#import "/utils/todo.typ": TODO

= Background
#TODO[
  Describe each proven technology / concept shortly that is important to understand your thesis. Point out why it is interesting for your thesis. Make sure to incorporate references to important literature here.
]

== Continuous Integration Systems

Continuous Integration (CI) is a software engineering practice in which developers frequently integrate small code changes into a shared mainline repository @mohammadContinuousIntegrationAutomation2016. The primary purpose of CI is to shorten feedback loops, reduce integration risks, and maintain the software in a consistently buildable state. These properties enable developers to deliver high-quality changes rapidly and reliably. As @shahinContinuousIntegrationDelivery2017a notes, that CI has become a foundational element of modern agile and DevOps-oriented development workflows, supports faster iteration and continuous feedback across the software lifecycle.

A typical CI workflow begins with developers committing changes to a version control system. The CI server continuously monitors the repository and automatically triggers a build when new changes appear. Automated unit and integration tests are executed to verify system correctness, CI system then produces build artifacts and reports results back to developers. This rapid validation cycle enables developers to identify defects, and reduce the complexity of integration work @stolbergEnablingAgileTesting2009.

To support these practices, a broad ecosystem of CI tools has emerged. Widely adopted open-source tools include Jenkins, GitLab CI, GitHub Actions, Travis CI, and CircleCI, each providing automated build, test execution, and pipeline capabilities. Enterprise solutions such as TeamCity and Bamboo offer more integrated commercial ecosystems. These systems collectively enable organizations to operationalize CI at scale and to embed automation deeply into their development processes @mohammadContinuousIntegrationAutomation2016.

== Kubernetes Fundamentals

As containerized applications became increasingly common, the operational challenge shifted from running individual containers to coordinating large collections of them across distributed environments. This need for automation led to the emergence of container orchestration systems, which provide mechanisms to deploy, scale, and manage containers in a cluster @Casalicchio2019. 

Jawarneh et al. describe container orchestration engines in terms of three core functional elements: resource management, scheduling, and service management @jawarnehContainerOrchestrationEngines2019. The resource-management layer handles low-level infrastructure concerns, including compute, memory, storage, and networking resources. The scheduling layer decides where containers should be placed in the cluster and supports mechanisms such as replication, scaling, and rescheduling. Finally, the service-management layer operates at a higher abstraction level and provides ready-to-use features such as grouping and namespacing. Building on this structural model, Khan highlights a set of capabilities that a modern orchestration platform should additionally enable, including high availability, security isolation between workloads, automated recovery from failures, service discovery, and support for continuous deployment @khanKeyCharacteristicsContainer2017. Together, these functional layers and operational capabilities characterize the essential requirements for managing complex containerized applications in a reliable and scalable manner.

Several container orchestration systems have been developed to realize these concepts in practice, including Docker Swarm, Apache Mesos, Rancher’s Cattle, and Kubernetes. Comparative studies indicate that while each system offers strengths for specific scenarios, Kubernetes distinguishes itself through its broad functional coverage and mature operational model @jawarnehContainerOrchestrationEngines2019. This combination has led to its widespread adoption and establishes it as the natural foundation for the system presented in this thesis.

At a high level, Kubernetes provides a declarative, API-driven architecture for managing containerized workloads. Users specify the desired state of applications through resource objects, and the Kubernetes control plane continuously works to reconcile the actual cluster state with this specification—a core principle highlighted in comparative analyses of orchestration engines @bernsteinContainersCloudLXC2014. The control plane consists of an API server that exposes a uniform interface for all cluster interactions, a scheduler that assigns Pods—the smallest deployable units that encapsulate one or more tightly coupled containers—to nodes, and a set of controllers that implement higher-level behaviors such as replication, rollout, and self-healing. Worker nodes, in turn, host a container runtime and node agents responsible for executing Pods and reporting their status back to the control plane @xuEmpiricalStudyKubernetes2024. This architecture not only realizes the functional elements and capabilities identified in prior work on container orchestration, but also provides an extensible foundation that can be adapted to domain-specific use cases, for example through custom controllers used to manage build jobs in continuous-integration scenarios.

== Kubernetes Operators and Custom Resource Definitions

Kubernetes provides a set of built-in resource types that cover common application needs, such as Pods, Deployments, and Services. However, many systems require domain-specific abstractions and lifecycle management that cannot be captured through the native APIs. To support such cases, Kubernetes allows its API to be extended by defining new resource types. As Xu et al. observe, Custom Resource Definitions (CRDs) have become the primary mechanism for introducing such domain-specific objects and integrating external systems into the Kubernetes control plane @xuEmpiricalStudyKubernetes2024.

A Custom Resource Definition (CRD) extends the Kubernetes API by introducing a new resource type. The CRD captures the structure of a domain-specific object by allowing users to define the values of the certain resource. Once the custom resources are registered with the API server, they can be created, inspected, and managed through the standard Kubernetes toolchain. This mechanisms allows the CRD to participate in declarative workflows, access control, and admission policies in the same way as native resources do. CRDs are the foundation upon which the Operator model builds: they make it possible for Kubernetes to treat domain-specific concepts as native objects, which enables higher levels of automation and lifecycle management @duanCaseStudyFive2021.

While CRDs define the API surface for a new resource, they do not by themselves implement any operational logic. Kubernetes achieves automation through controllers, which continuously compare the desired state specified in custom resources with the actual cluster state and take steps to reconcile the two. When domain-specific logic is encoded in such a controller and paired with a CRD, the combination is known as the Operator Pattern. Originally introduced by CoreOS, the Operator pattern enables Kubernetes to manage complex systems through custom control loops that encapsulate operational knowledge. Operator Pattern allows automate tasks such as deployment, configuration, scaling, monitoring, and failure recovery without manual intervention @henningReproducibleBenchmarkingCloudNative. Xu et al. further observe that Operators have become a widely adopted mechanism for managing sophisticated cloud-native applications, reflecting their ability to embed domain expertise directly into Kubernetes's reconciliation model @xuEmpiricalStudyKubernetes2024.