#import "/utils/todo.typ": TODO

= Related Work
// #TODO[
//   Describe related work regarding your topic and emphasize your (scientific) contribution in contrast to existing approaches / concepts / workflows. Related work is usually current research by others and you defend yourself against the statement: “Why is your thesis relevant? The problem was al- ready solved by XYZ.” If you have multiple related works, use subsections to separate them.
// ]

== Container Orchestration & Cloud-Native Infrastructure

The evolution of cloud computing infrastructure has witnessed a paradigm shift from traditional Virtual Machines (VMs) to container-based virtualization. While VMs rely on hypervisors to provide complete operating system isolation, containers evolved from Linux Containers to the standardized Docker ecosystem, sharing the host operating system's kernel, which reduces resource overhead and startup time. @bernsteinContainersCloudLXC2014. Docker revolutionized software delivery by enabling a "build, ship, and run anywhere" model, ensuring that applications and their libraries are bundled into isolated, immutable environments @shahBuildingModernClouds2019.

However, while containers address the portability issue for individual applications, deploying them at scale introduces considerable operational complexity. Modern applications often consist of hundreds of microservices with complex interdependencies, making manual management of container lifecycles and resource allocation infeasible @jawarnehContainerOrchestrationEngines2019. Furthermore, standalone containers function as separate entities without inherent communication lines, creating a need for a higher-level management layer @shahBuildingModernClouds2019. Research indicates that cluster management complexity grows exponentially $O(n^2)$ with the number of nodes and containers, necessitating an automated orchestration platform to integrate and manage these workloads at scale @khanKeyCharacteristicsContainer2017.

To support modern microservice architecture, @khanKeyCharacteristicsContainer2017 suggests a container orchestration platform must provide a specific set of capabilities, including cluster state management, high availability, security, networking, service discovery, and monitoring. Among the available solutions, Kubernetes has emerged as the de facto standard for container orchestration by decoupling application containers from underlying infrastructure details @bernsteinContainersCloudLXC2014. In a thorough functional and performance comparison conducted in @jawarnehContainerOrchestrationEngines2019, Kubernetes proves to be the most complete orchestrator compared to Docker Swarm and Apache Mesos, outperforming its counterparts in handling complex application deployments. Kubernetes offers ready-to-use features such as automatic packing for resource optimization, self-healing mechanisms that automatically restart failed containers, and secret management for secure configuration @shahBuildingModernClouds2019. These characteristics make Kubernetes an ideal infrastructure for executing complex, scalable CI/CD workflows.

== Evolution of CI/CD: From Agile to Cloud-Native

The evolution of Continuous Integration has been essential in shifting software development from manual processes to automated workflows. In the early stages, the "gatekeeper" model, which relies on manual code verification, was identified as a critical bottleneck. Lacoste demonstrated that replacing this manual role with an automated CI system could effectively eliminate integration lag @lacosteKillingGatekeeperIntroducing2009. This automation was not only an efficiency upgrade but a fundamental enabler for Agile methodologies. Without the immediate feedback loop provided by CI, testing teams often revert to waterfall practices even within Agile iterations. Stolberg argues that CI is the prerequisite for agile testing, as it transforms testing from a manual phase into a continuous activity that runs in parallel with development @stolbergEnablingAgileTesting2009. Furthermore, CI is widely recognized as the foundational practice for DevOps automation, requiring not just tool implementation but a cultural shift to ensure that integration errors are detected and resolved immediately @mohammadContinuousIntegrationAutomation2016.

As infrastructure paradigms shifted, CI/CD pipelines evolved to leverage cloud-native technologies, particularly container orchestration. Mustyala highlights that implementing CI/CD pipelines within Kubernetes environments significantly accelerates software development by standardizing deployment processes across the software development lifecycle. By utilizing Kubernetes for orchestration, organizations can create robust pipelines that support auto-scaling and self-healing, ensuring that the CI/CD infrastructure itself benefits from the same reliability and efficiency as the applications it deploys @mustyalaCICDPIPELINES.

== Scalable Build Systems & Scheduling Optimization

The architecture of Continuous Integration (CI) systems has evolved to meet varying scalability demands. In the context of programming exercise environments, Jandow proposed HadesCI, a scalable system designed to handle high-volume concurrent submissions @jandowHadesCIScalableContinuous. HadesCI addresses scalability by using a producer-consumer model backed by Redis, dynamically dispatching build jobs to ephemeral Docker containers rather than relying on static build agents. While HadesCI focuses on Docker-based execution, the broader industry has increasingly adopted Kubernetes for orchestration. In this domain, Shevchuk et al. addressed the security challenges inherent in shared Kubernetes-based CI/CD pipelines. They proposed a hardened architecture that strictly enforces the principle of least privilege. By implementing granular Role-Based Access Control and restrictive network policies, their approach effectively segments build execution environments, thereby mitigating the risk of privilege escalation where a compromised build container could attack the underlying cluster infrastructure @shevchukSoftwareImproveSecurity2023.

Scaling build systems for massive industrial enterprises introduces complexity beyond execution isolation. Esfahani et al. presented CloudBuild, Microsoft's solution to the inefficiencies caused by diverse legacy tools and fragmented infrastructure @esfahaniCloudBuildMicrosoftsDistributed2016a. A core innovation of CloudBuild is its aggressive Content-Based Caching and Dependency Prediction, which uses a file access monitor based on the Detours library to automatically discover hidden dependencies and ensure correctness. Furthermore, CloudBuild employs a sophisticated DAG-based scheduling strategy to optimize execution throughput. The system constructs a fine-grained dependency graph to identify the critical path of the build process. The scheduler then prioritizes the execution of these critical tasks, ensuring that blocking dependencies are resolved as early as possible to minimize the total end-to-end latency.

While CloudBuild focuses on caching and correctness, efficient resource management in centralized build farms is equally critical. Wang et al. from Google proposed a Smart Scheduling Service to manage millions of daily builds under finite resource constraints @wangScalableBuildService2020a. This system introduces a Resource Aware Scheduling mechanism that utilizes a Build Cost Prediction Model to estimate execution costs (such as CPU and memory) based on historical build data. This enables the scheduler to efficiently allocate builds to machines using a greedy algorithm rather than a random assignment. To maintain stability during peak loads, they applied control theory by implementing a PID (proportional-integral-derivative) controller for Adaptive Flow Control. This controller dynamically adjusts the maximum concurrency based on real-time feedback signals such as loop latency, ensuring the system maximizes throughput without overloading the infrastructure.
